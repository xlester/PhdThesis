\chapter{Introduction to fringe pattern demodulation}

%%\section{Introduction}

A fringe pattern is defined as a sinusoidal signal where a continuous map, 
analogous of the physical quantity being measured, is phase-modulated by an 
interferometer, as a Moire system. An ideal stationary fringe pattern is usually
modeled as:
\begin{equation}\label{eq:FringePattern}
I(x,y)=a(x,y)+b(x,y)cos[\phi(x,y)],
\end{equation}
where $a(x,y)$ and $b(x,y)$ are the background and local contrast functions,
respectively; and $\phi(x,y)$ is the physical searched phase function. The spatial
dependence $(x,y)$ represents the position of the pixel in the image.

Analyzing Eq. \eqref{eq:FringePattern} one can see that the phase function
$\phi(x,y)$ cannot be directly estimated since it is screened by two other
functions, $a(x,y)$ and $b(x,y)$. Additionally, $\phi(x,y)$ can only be
determined modulo $2\pi$ because the sinusoidal fringe pattern $I(x,y)$ depends
periodically on the phase ($2\pi$ phase ambiguity); and its sign cannot be 
extracted from a single measurement without a priori knowledge (sign ambiguity)
due to even character of the cosine function [$cos(\phi)=cos(-\phi)$]. Finally, 
in all practical cases some noise $\eta(x,y)$ is introduced in an additive and/or
multiplicative way, and the fringe pattern may suffer from a number of distortions,
degrading its quality and further screening the phase information 
\cite{Schwider:83,Creath:91}.

It must be noted that even if careful experimental setups could prevent the
screening of $\phi(x,y)$ due to the unknown signals $a(x,y)$, $b(x,y)$ and $\eta
(x,y)$, we would still have to deal with the sign ambiguity and the $2\pi$ phase
ambiguity. Because of these ambiguities, the solution for this inverse problem is
not unique; this is because several phases can produce exactly the same sinusoidal
signal.

If we rewrite Eq. \eqref{eq:FringePattern} by means of the complex representation of
the cosine function, we have:
\begin{equation}\label{eq:FringePatternExp}
 I(x,y)=a(x,y)+ \frac{1}{2} b(x,y)[e^{i \phi(x,y)} + e^{-i \phi(x,y)}],
\end{equation}
and if we are able to isolate one of the analytic signals in Eq. 
\eqref{eq:FringePatternExp}, lets say $\frac{1}{2} b(x,y)e^{i \phi(x,y)}$, we 
obtain:
\begin{equation}\label{eq:TanPhi}
 \tan \phi(x,y)=\frac{Im\{(1/2)b(x,y)e^{i \phi(x,y)}\}}{Re\{(1/2)b(x,y)
e^{i \phi(x,y)}\}}.
\end{equation}
Computing the arc-tangent of the above formula we obtain a wrapped estimation of
the phase under study: $\phi(x,y)$ mod $2\pi$. Thus, the final step of this fringe
pattern demodulation process usually involves an additional phase unwrapping 
process \cite{Itoh:82,Huntley:89,Ghiglia:94,Quiroga:94}. Nevertheless, when 
working with an smooth phase $\phi$ this last step is 
straightforward. Next we will illustrate the easiest way to obtain these analytic 
signals \cite{ServinA:10,Crane:69,Hays:71}.

\section{Fringe pattern with temporal carrier}

A fringe pattern obtained as the output of a measuring system may be modified
by the optoelectronic-mechanical hardware (sensors and actuators) and software
(virtual sensors and actuators) of the system \cite{Kujawinska2006}. With these 
modifications we are able to introduce known changes in the argument of the 
sinusoidal signal:
\begin{equation}\label{eq:FringePatternTemporal}
 I(x,y,k)=a(x,y)+b(x,y)cos[\phi(x,y) + \c{C}(x,y,k)],
\end{equation}
where $\c{C}(x,y,k)$ is a known function (typically a reference plane) and it is 
called the spatio-temporal carrier of the interferogram. The spatial and/or 
temporal carriers are of extreme importance in modern interferometry: first of 
all, its presence allows to solve the sign ambiguity since in general 
$\cos(\phi+\c{C})\not=\cos(-\phi+\c{C})$. They also allow to isolate the analytic 
signal
$\frac{1}{2} b(x,y)e^{i \phi(x,y)}$. In phase-shifting interferometry (PSI) the
linear temporal carrier can be written as:
\begin{equation}\label{PSIcarrier}
 \c{C}(x,y,k)=\alpha k,
\end{equation}
where $\alpha$ is the temporal lineal carrier, typically $\alpha=\pi/2$, and $k$
is the discrete temporal variable. The independent temporal variable $k$ represents
the $k$-frame of the PSI sequence. In this context, knowing the temporal carrier 
$\alpha$ (phase step as known in PSI), the objective of the PSI demodulation 
methods is to estimate the complex field
\begin{equation}\label{eq:Complexfield}
 f(x,y)=\frac{1}{2}b(x,y)e^{i\phi(x,y)},
\end{equation}
of the interferometric image or images at the $(x,y)$ pixel. Then, the optical 
phase at $(x,y)$ is obtained as:
\begin{equation}\label{eq:PSIphase}
  \phi(x,y)=arg[f(x,y)]=\arctan\left(\frac{\sin[\phi(x,y)]}{\cos[\phi(x.y)]}
\right),
\end{equation}
which is an alternative form of Eq. \eqref{eq:TanPhi}.

In the following sections we will analyze several methods to estimate the 
analytic signal $f(x,y)$.

\section{Phase-shifting algorithms (PSAs)}

In general, a phase-shifting algorithm (PSA) can be described as quadrature linear
filter which is completely characterized by its impulse response function, $h(k)$,
or by its frequency transfer function (FTF) in the Fourier domain, $H(\omega)$:
\begin{equation}
 h(k)=\sum^{N-1}_{n=0} c_n \delta(k-n),
\end{equation}
\begin{equation}
 H(\omega)=\sum^{N-1}_{n=0} c_n e^{-i\omega n},
\end{equation}
where ${c_n} \in \mathbb{C}$. In order to be a valid PSA, this FTF must fulfill
the so-called quadrature conditions:
\begin{equation}\label{eq:QuadratureConditions}
 H(0)=H(-\omega_0)=0, \quad H(\omega)\not=0.
\end{equation}
Thus, the application of this quadrature filter produces the following analytic
signal for $k = N-1$ (where all the available data is involved):
\begin{equation}\label{eq:PSA_convolution2}
 I(k)*h(k)=I(k)*\sum^{N-1}_{n=0} c_n \delta(k-n) = \sum^{N-1}_{n=0} c_n I(k-n),
\end{equation}
where $*$ denotes the convolution operator. Taking the Fourier transform of Eq. 
\eqref{eq:PSA_convolution2} and applaying the convolution theorem and the 
quadrature conditions from Eq. \eqref{eq:QuadratureConditions}, results:
\begin{equation}\label{eq:PSA_FourierConvolution}
 I(\omega)H(\omega)= \frac{1}{2} H(\omega_0)b(x,y) e^{i\phi(x,y)} \delta(\omega-
\omega_0).
\end{equation}
Returning to the temporal domain and combining Eqs. 
\eqref{eq:PSA_convolution2} and \eqref{eq:PSA_FourierConvolution}, the estimated 
analytic signal is given by:
\begin{equation}
 I(k)*h(k)= \sum^{N-1}_{n=0} c_n I(k-n) = \frac{1}{2} H(\omega_0)b(x,y) 
e^{i(\phi(x,y)+k\omega_0)}.
\end{equation}
Considering the above, we have to evaluate the temporal convolution at $k=N-1$ in
order to obtain the analytic signal for the given number of phase steps:
\begin{equation}
  \frac{1}{2} H(\omega_0)b(x,y) e^{i\phi(x,y)} = c_0 I_{N-1} + c_1 I_{N-2}+ \dots 
  + c_{N-2} I_{1}+ c_{N-1} I_{0}.
\end{equation}
This is the general formula for a linear PSA.
From the above analytic signal, we obtain the searched phase $\phi(x,y)$, modulo
$2\pi$, by computing its angle as Eq. \eqref{eq:PSIphase} \cite{Surrel:96,
Wyant:75,Kujawinska2006}. 
For completeness, 
solving for $\phi(x,y)$ is found the so-called arc-tangent formulation of a PSA:
\begin{equation}
 \phi(x,y) = \arctan \Bigg(\frac{Im\{c_0I_0+c_1I_1 + \dots + c_{N-1}
I_{N-1}\}}{Re\{c_0I_0+c_1I_1 + \dots + c_{N-1} I_{N-1}\}}  \Bigg).
\end{equation}
Recall that the estimated phase is wrapped modulo $2 \pi$.
The above results are obtained under the assumption that the quadrature filter
and the temporal samples of the interferogram are perfectly tuned at the same
frequency $\omega$ (radians per sample) \cite{Gonzalez:11}.

\subsection{Detunin error in PSI}
When the actual sampling frequency is
$\alpha+\Delta$, or in other words when the phase step is not as expected; 
that is when some detuning error occur, the resulting phase estimation is given by
\begin{equation}
 \hat{\phi}(x,y)=\phi(x,y)-D(\Delta)\sin[2\phi(x,y)],
\end{equation}
where the amplitude of the detuning-error is given by the ratio
\begin{equation}
 D(\Delta)=\frac{|H(-\omega_0-\Delta)|}{|H(\omega_0+\Delta)|},
\end{equation}
for $|\Delta/\alpha|<<1$ \cite{Mosino:09,Mosino:10}.This equation shows that 
detuning-error causes the estimated phase to be distorted by a component with 
twice the original fringes frequency. The ratio allows us to assess the robustness 
against detuning error of any liinear PSA with a simple plot of $D(\Delta)$ versus
$\Delta$. Over the years, several linear PSAs with robustness against detunind 
error have been proposed \cite{Schwider:83,Surrel:93,Surrel:97,Hariharan:87,
Larkin:92,Schmit:95,Schmit:96,Hibino:97,deGroot:95,Bi:04}. The first one (and 
probably the best known) is the Schwider-Hariharan five-step PSA \cite{Schwider:83,
Hariharan:87}.

\subsection{Noise in PSI}
Whenever a linear PSA is applied it is important to know how robust it will be 
when non-ideal conditions arise. Over the years several non-spectral analysis 
have been reported for phase-shifting demodulation of noisy data: using Taylor 
expansion of the fringe irradiance \cite{Brophy:90}, joint
statistical distribution of the noise \cite{Rathjen:95}, the characteristic 
polynomial method \cite{Surrel:97}, the derivative of the PSA's arctangent ratio 
\cite{Hibino:97} and so on. Here we present a theoretical analysis on the 
influence of additive random-noise into the modulating-phase estimation 
accordingly with the FTF formalism and the stochastic process theory 
\cite{Papoulis:2002}.
In fringe pattern analysis one must deal with two kinds of noise: additive
noise, which comes from the ambient and the electronic equipment used; and
multiplicative (or speckle) noise, observed when testing optically rough surfaces.

According to diffraction theory, the phase noise results from the
interference of light scattered from each point of an optically rough surface 
\cite{Gasvik:2002}. While this effect can be considered an unwanted distortion, 
it can also be exploited as a measuring tool; for instance, consider the 
electronic speckle pattern interferometry (ESPI) technique \cite{Gasvik:2002}.
However, since the information of interest in closed-fringes interferograms is 
given by low-frequency signals, in
practice one usually pre-processes any set of phase-shifted patterns by applying
some spatial low-pass filtering to improve its signal-to-noise ratio. This spatial
low-pass filtering modifies the statistical properties of the multiplicative noise
into additive Gaussian noise. The theoretical foundation for this change in the
statistical properties of the noise is the central limit theorem 
\cite{Papoulis:2002}. For our
purposes, this theorem says that the output signal resulting from the linear
filtering of a stochastic (random) process with finite mean and variance tends
to Gaussian statistics, no matter which statistical distribution the input process
had. This means that, whether having additive and/or multiplicative noise, the
low-pass filtering will turn the output noise more Gaussian and additive. For
this reason, in our following analysis we will consider only additive noise
\cite{Servin:11,Surrel:97}.

Interferometric signals are always distorted by some amount of random
noise so a more cautious approach it is to consider additive white-noise 
distortion (with random components in every spectral frequency). Under this 
assumption, when filtering with quadrature linear filters, the signal-to-noise 
power ratio gain of the PSA is given by \cite{Servin:09}:
\begin{equation}
 G_{S/N}(\omega_0)=\frac{|H(\omega_0)|^2}{\frac{1}{2\pi}\int\limits_{-\infty}
^{\infty} |H(\omega)H^*(\omega)|^2 d\omega}.
\end{equation}
When $G_{S/N}(\omega_0)>1$, the output has a higher S/N ratio than the input data;
the standard case. When $G_{S/N}(\omega_0)=1$, the output analytic signal has 
the same S/N power as the interferograms. Finally when $G_{S/N}(\omega_0)<1$, the 
searched analytic signal has a lower S/N than the input; this situation is not 
desired.

\section{Classical least-square algorithm}

The N-step least-squares phase-shifting algorithm formula was deduced in
1974 by Bruning et al. \cite{Bruning:74} following a synchronous detection 
technique for the phase-demodulation of temporal phase-shifted interferograms. 
Later in 1982 C. J. Morgan \cite{Morgan} demonstrated that this family of PSAs 
correspond to the principle of least-squares estimation under the presence of 
external perturbations. Finally, J. E. Greivenkamp \cite{Greivenkamp} shown in 
1984 that this least-squares fit provides the better phase-estimation against 
non-uniform phase-steps.

In a PSI sequence each $(x,y)$ pixel is a 1D temporal discrete interferometric 
signal modeled as Eq. \eqref{eq:FringePatternTemporal}. Using the carrier 
$\c{C}(x,y,k)=\alpha k$, we have the following expression
\begin{eqnarray}
  I(x,y,k) &=& a(x,y) + b(x,y)\cos[\phi(x,y) +\alpha k] \nonumber \\
  &=& a(x,y) + c(x,y)\sin[\alpha k] - s(x,y)\cos[\alpha k],
  \label{eq:PSI_Sequence}
\end{eqnarray}
where $c(x,y)=b(x,y)\cos[\phi(x,y)]$ and
$s(x,y)=b(x,y)\sin[\phi(x,y)]$ are the quadrature components of the
1D temporal interferometric signal, $k$ is the discrete temporal
variable and $\alpha$ the phase step or temporal carrier; note that
all these variables are scalars. The independent temporal variable $k$
represents the $k$-frame of the PSI sequence. In this context, knowing
the phase step $\alpha$, the objective of the least-squares algorithm is to 
estimate the quadrature components $c(x,y)$ and $s(x,y)$ of the interferometric
signal at the $(x,y)$ pixel. Then, the phase at $(x,y)$ is obtained as 
mentioned above in Eq.\eqref{eq:PSIphase}.

Scanning all pixels in this way, we obtain the wrapped phase image of
the PSI sequence. One of the first approaches to demodulate a PSI
sequence was the least-squares model for PSI
\cite{Morgan,Greivenkamp,Okada,Kong}. The least-squares model (cost function) 
for PSI is the following,
\begin{align}\label{eq:PSIleast-squares}
  U[a,c,s]= \sum_{k=0}^{N-1}\left[a + c \sin(\alpha k)\right.
  -\left. s \cos(\alpha k)-I(k) \right]^2,
\end{align}
where $I(k)$ is the observed value of the $k$-frame at the
$(x,y)$ pixel modeled as in Eq. \eqref{eq:PSI_Sequence}. For clarity, 
the spatial dependence $(x,y)$ is omitted in parameters $a,c,s$ and $I$.
To have a well-posed mathematical model for Eq. \eqref{eq:PSIleast-squares},
it is necessary to have at least three interferograms in the PSI sequence;
that is, $N\geq 3$. The parameters $c(x,y)$ and $s(x,y)$ that minimize Eq. 
\eqref{eq:PSIleast-squares} are the quadrature components used in Eq. 
\eqref{eq:PSIphase} to obtain the phase. To minimize Eq. 
\eqref{eq:PSIleast-squares} we need to find
\begin{equation}
 \partial U/ \partial a(x,y)=0; \partial U/ \partial c(x,y)=0; 
\partial U/ \partial s(x,y)=0;
\end{equation}
that yields as solution
\begin{equation}
 X(x,y)=A^{-1}B(x,y)
\end{equation}
where $X(x,y)$ and $B(x,y)$ are a $3 \times 1$ vectors for every pixel given by
\begin{equation}\label{eq:LQX}
 X(x,y)=[a(x,y),c(x,y),s(x,y)]^T
\end{equation}
and
\begin{equation}
 B(x,y)=\bigg[\sum_k I(x,y,k),\sum_k I(x,y,k) \cos(\alpha k),
 \sum_k I(x,y,k) \sin(\alpha k) \bigg]^T
\end{equation}
and the matrix A does not depend on the position and is given by
\begin{equation}
A=\left[\begin{array}{ccc}
N & \sum_{k}cos(\alpha k) & \sum_{k}sin(\alpha k)\\
\sum_{k}cos(\alpha k) & \sum_{k}cos^{2}(\alpha k) & \sum_{k}cos(\alpha k)
\sum_{k}sin(\alpha k)\\
\sum_{k}sin(\alpha k) & \sum_{k}cos(\alpha k)\sum_{k}sin(\alpha k) & 
\sum_{k}sin^{2}(\alpha k)\end{array}\right].
\end{equation}
Matrix A needs at least three different phase shifts to be non-singular (and 
invertible) and make possible the computation of the solution $X(x,y)$ for every
pixel. From Eq. \eqref{eq:LQX} the wrapped phase for each pixel is calculated as
\begin{equation}
 \phi(x,y)=\arctan\left[\frac{-s(x,y)}{c(x.y)} \right].
\end{equation}
Matrix A must be calculated only once for frame, in consequence as the temporal 
interferometric signal of each pixel has the same model 
[see Eq. \eqref{eq:PSI_Sequence}], the solution of the linear equation system is 
always the same.

\section{Robust quadrature filters (RQF) for PSI}

Regularization systems are very useful full-field systems that can use all the 
information needed to obtain the data sought as expected. In PSI, we can use 
these techniques to include the temporal and spatial information to recover the 
modulating phase as a smooth 2D function, removing unwanted harmonics and noise. 
Actually, regularization techniques have been used before in PSI for these
purposes, the first were Marroquin et al. \cite{RQF,AQF,AQF_mult,Marroquin1999} 
and more recently others \cite{RPT, RQPT, Mariano, Mariano2, Vargas,Medina,Zeng,
Servin2001,Legarda-Saenz:02,Tian2010,Quiroga,Kai:12,Legarda-Saenz:06}.

The main idea behind Robust Quadrature Filters (RQF) for PSI is to find a 
complex field
\begin{equation}
 f = \varphi +i \psi
\end{equation}\label{eq:RQF}
that minimizes the next cost function
\begin{equation}
 U(f)=U_D(f,I)+ \lambda U_R(f).
\end{equation}
The first term $U_D(f,I)$ is commonly known as the data term, and it depends on
the difference between the observed data I (in this case the interferogram 
without the background term) and the estimation model $f$, in such way that $U_D(f,I)$ is 
minimal when $f$ is close to $I$. $U_D(f,I)$ can be defined
\begin{equation}
U_D(f,I)=\sum_{(x,y) \in L} \| f(x,y)- I(x,y) \| ^2.
\end{equation}
The second term $U_R(f)$ is usually called the regularization term. This term 
imposes smoothness to the modulating phase $\phi$ adding restrictions to the
estimation model $f$. Two regularization terms are the first-order and 
second-order regularization operators known as membrane model and thin-rod model,
respectively. For the membrane model we have the following cost function
\begin{equation}
 U_{Rm} = \lambda \sum_{(x,y)\in L} \{\| f(x,y)-f(x-1,y) e^{iu_0}\|^2 + 
\| f(x,y)-f(x,y-1) e^{iv_0} \|^2 \},
\end{equation}
and for the thin-rod model the cost function is 
\begin{small}
\begin{multline}
 U_{Rt} = \\ \lambda \sum_{(x,y)\in L} \{ 
            \| f(x,y)-f(x-1,y) e^{ iu_0} + 
                 f(x,y)-f(x+1,y) e^{-iu_0}\|^2 + \\ 
         \| f(x,y)-f(x,y-1) e^{ iv_0} + 
                 f(x,y)-f(x,y+1) e^{-iv_0}\|^2 \} + \\
         \| f(x,y)-f(x-1,y-1) e^{ iv_0} + 
                 f(x,y)-f(x+1,y+1) e^{-iv_0}\|^2 \}.
\end{multline}
\end{small}
In both cases $u_0$ is the tuning frequency in $x$, and $v_0$ is the tuning 
frequency in $y$. The parameter $\lambda$ is known as the regularization parameter
that controls the bandwidth of the quadrature filter.

To minimize Eq. \eqref{eq:RQF}, we obtain its gradient with respect to $f$ and
equal it to zero \cite{RQF,AQF,AQF_mult}. Then, we can solve the resulting linear equation system using
algorithms such as the Gauss-Seidel algorithm, although we can use more generic 
algorithms, like the Steepest-descent. Once we find the complex field $f$ we 
obtain the optical phase using Eq. \eqref{eq:PSIphase}.
